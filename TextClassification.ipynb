{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='steelblue'>\n",
    "\n",
    "<span style=\"font-family:verdana; font-size:1.6em;\">\n",
    "    <strong>Text Classification Using Neural Network</strong><br>\n",
    "    Use the movie review dataset for sentiment analysis<br>\n",
    "</span>\n",
    "<span style=\"font-family:verdana; font-size:1.4em;\">\n",
    "    <b>Following examples are included in the processing:</b><i>\n",
    "    <ol>\n",
    "        <li>Load dataset from csv file</li>\n",
    "        <li>Preprocess data</li>\n",
    "        <li>Create training and test dataset</li>\n",
    "        <li>Tokenize the sequences</li>\n",
    "        <li>Explore the tokenization</li>\n",
    "        <li>Create a neural network</li>\n",
    "        <li>Train model and make predictions</li>\n",
    "        <li>Explore model performance</li>\n",
    "    </ol> </i>   \n",
    "</span>\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import timeit, time\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "#from tensorflow.keras import losses\n",
    "from tensorflow.keras import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50,000 movie reviews labeled as \"positive\" or \"negative\"\n",
    "df = pd.read_csv(\"../datasets/IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the sentiment (target variable) to numbers\n",
    "df[\"sentiment\"] = df[\"sentiment\"].map({\"positive\": 1, \"negative\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess function\n",
    "<span style=\"font-family:verdana; font-size:1.2em;\">\n",
    "    Preprocess the input review to do the following:\n",
    "    <ol>\n",
    "        <li>Convert to lower case</li>\n",
    "        <li>Remove html tags if found</li>\n",
    "        <li>Remove puncutations</li>\n",
    "    </ol>\n",
    "</span>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "\n",
    "def preprocessData(input_data):\n",
    "    lowercase = input_data.lower()\n",
    "    stripped_html = lowercase.replace('<br />', ' ')\n",
    "    retval = re.sub(r'[^\\w\\s]','', stripped_html)\n",
    "    return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the review column\n",
    "df['review'] = df['review'].map(preprocessData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df[\"review\"].values[:25000]\n",
    "X_test = df[\"review\"].values[25000:]\n",
    "\n",
    "y_train = df[\"sentiment\"].values[:25000]\n",
    "y_test  = df[\"sentiment\"].values[25000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# define hyper parameters that can be modified\n",
    "vocab_size = 10000\n",
    "embedding_dim = 16\n",
    "# If a sentence is shorter than max_length it will be padded, longer sentences will be truncated\n",
    "max_length = 250  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Out of Vocabulary token rather than throwing away unknown words\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token = \"<oov>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert words to numbers and pad for the neural network to use as input\n",
    "# if the sequence length is greater than max length then truncate it at the end\n",
    "# if the sequence length is less than max length then pad it at the end\n",
    "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, \n",
    "                             padding = \"post\", truncating=\"post\")\n",
    "\n",
    "\n",
    "# tokenized using the word_index learned from the training data\n",
    "testing_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "test_padded = pad_sequences(testing_sequences, maxlen=max_length, \n",
    "                            padding = \"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padded[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverses keys: keys become the values, and values become the keys so that \n",
    "# we can look a word up (display padded as ?)\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "\n",
    "# This is what will be fed in\n",
    "print(decode_review(train_padded[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the original text\n",
    "print(X_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model\n",
    "<span style=\"font-family:verdana; font-size:1.2em;\">\n",
    "    Model Parameters:\n",
    "    <ol>\n",
    "        <li><b>Embedding: </b>\n",
    "            <ul>\n",
    "                <li>Embedding layer stores one vector per word</li>\n",
    "                <li>Converts sequences of word indices to sequences of vectors</li>\n",
    "                <li>These vectors are trainable</li>\n",
    "                <li>Once trained, words with similar meanings often have similar vectors</li>\n",
    "                <li>This approach is more efficient than using a dense layer with one hot encoding</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li><b>GlobalAveragePooling1D: </b>\n",
    "            <ul>\n",
    "                <li>Returns a fixed length output vector for each example by averaging over the sequence dimension</li>\n",
    "                <li>Allows the model to handle input of variable length</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li><b>Couple of Dense layers: </b>\n",
    "            <ul>\n",
    "                <li>Apply the dense layer with ReLU activation</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ol>\n",
    "    The last output layer use sigmoid to get probability of positive or negative sentiment\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = tf.keras.Sequential([\n",
    "    # The Embedding layer is the key to text sentiment analysis\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(8, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timer for measuring time taken to train the model\n",
    "start = timeit.default_timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_padded, y_train, epochs=5, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop timer and print training time\n",
    "stop = timeit.default_timer()\n",
    "execution_time = stop - start\n",
    "exectime = time.strftime(\"%M:%S\", time.gmtime(execution_time)) \n",
    "print(\"To train it took: {} mins\".format(exectime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output from the Embedding layer\n",
    "embeddings = model.layers[0]\n",
    "\n",
    "weights = embeddings.get_weights()[0]\n",
    "print(f\"Vocabulary size: {weights.shape[0]},  Embedding dimensions: {weights.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the shape is: \n",
    "# (the number of words in the corpus, the embedding dimensions)\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "# writing the vectors and their metadata out to file. \n",
    "# these 2 files ('vecs.tsv', 'meta.tsv') are used by the \n",
    "# TensorFlow projector (http://projector.tensorflow.org/)\n",
    "# to plot/visualize the vectors/embeddings in 3D\n",
    "\n",
    "# Output of the 16 values per word representation (embedding)\n",
    "#      out_v are the weights (embedding)\n",
    "#      out_m are the actual words associated with each embedding\n",
    "\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "for word_num in range(1, vocab_size):\n",
    "  word = reverse_word_index[word_num]\n",
    "  embeddings = weights[word_num]\n",
    "  out_m.write(word + \"\\n\")\n",
    "  out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.history['val_'+ string])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string, 'val_'+string])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(history,'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_padded, y_test)\n",
    "\n",
    "print(\"Test accuracy: \", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on a positive sample\n",
    "\n",
    "sample_text_to_predict = [\"The movie was great. The animation and the graphics was excellent. I would recommend this movie.\"]\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(sample_text_to_predict)\n",
    "pos_padded = pad_sequences(train_sequences, maxlen=max_length, padding = \"post\", truncating=\"post\")\n",
    "\n",
    "#  make prediction\n",
    "prediction = model.predict(pos_padded)\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on a negative sample\n",
    "\n",
    "sample_text_to_predict = [\"The movie was horrible. The animation and the graphics were wrost. I would not recommend this movie.\"\n",
    "]\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(sample_text_to_predict)\n",
    "neg_padded = pad_sequences(train_sequences, maxlen=max_length, padding = \"post\", truncating=\"post\")\n",
    "\n",
    "#  make prediction\n",
    "prediction = model.predict(neg_padded)\n",
    "\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
