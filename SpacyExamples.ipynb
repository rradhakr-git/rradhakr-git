{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='steelblue'>\n",
    "<h2>\n",
    "<span style=\"font-family:verdana; font-size:1.2em;\">\n",
    "Using Spacy to explore different tools that are supported in text processing<br>\n",
    "</span>\n",
    "</h2>\n",
    "</font>\n",
    "\n",
    "<font color='grey'>\n",
    "<span style=\"font-family:verdana; font-size:1.2em;\">\n",
    "<b>\n",
    "Look at following examples:\n",
    "    <ul>\n",
    "        <li>Loading a prebuild model</li>\n",
    "        <li>Tokenization</li>\n",
    "        <li>Text preprocessing</li>\n",
    "        <li>Lemmatization</li>\n",
    "        <li>Vocabulary</li>\n",
    "        <li>Lexical analysis</li>\n",
    "        <li>Parts of speech analysis</li>\n",
    "        <li>Named entities within the text</li>\n",
    "     </ul>\n",
    "    </b>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install spacy<br>\n",
    "<font color='tomato'>\n",
    "<span style=\"font-family:verdana; font-size:1.4em;\">\n",
    "    Couple of options to install spacy:\n",
    "    <ul>\n",
    "        <li>conda install -c conda-forge spacy</li>\n",
    "        <li>pip install spacy</li>\n",
    "    </ul><br>\n",
    "    In anaconda terminal use the following commands to download small model or large model:\n",
    "    <ul>\n",
    "        <li>python -m spacy download en_core_web_sm</li>\n",
    "        <li>python -m spacy download en_core_web_lg</li>\n",
    "    </ul>\n",
    "        \n",
    "</span>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='gray'>\n",
    "<h3>\n",
    "<span style=\"font-family:verdana; font-size:1.2em;\">\n",
    "spaCy - Functionality<br>\n",
    "</span>\n",
    "</h3>\n",
    "</font>\n",
    "<span style=\"font-family:verdana; font-size:1.0em;\">\n",
    "spaCy comes with predefined NLP models that can be used to perform most common tasks:<br>\n",
    "    <ul>\n",
    "        <li>Tokenization</li>\n",
    "        <li><a href=\"https://spacy.io/usage/linguistic-features\" target=\"_blank\">Parts of speech (POS) tagging</a></li>\n",
    "        <li><a href=\"https://www.machinelearningplus.com/nlp/training-custom-ner-model-in-spacy/\" target=\"_blank\">Named Entity Recognition (NER) </a></li>\n",
    "        <li><a href=\"https://spacy.io/api/lemmatizer\" target=\"_blank\">Lemmatization</a></li>\n",
    "        <li><a href=\"https://spacy.io/usage/vectors-similarity\" target=\"_blank\">Transforming words into vectors</a></li>\n",
    "    </ul>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load small english model (returns an object)\n",
    "model = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"\"\"The economic situation of the country is on edge , as the stock \n",
    "market crashed causing loss of millions. Citizens who had their main investment \n",
    "in the share-market are facing a great loss. Many companies might lay off \n",
    "thousands of people to reduce labor cost\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Create a document object using the model for our text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Doc object\n",
    "doc1 = model(text1)\n",
    "print(type(doc1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='gray'>\n",
    "<h3>\n",
    "<span style=\"font-family:verdana; font-size:1.2em;\">\n",
    "spaCy - Tokenization<br>\n",
    "</span>\n",
    "</h3>\n",
    "</font>\n",
    "<span style=\"font-family:verdana; font-size:1.2em;\">\n",
    "    <ul>\n",
    "<li>Tokens are individual text entities that make up the document. They are words, punctuations, spaces, etc</li>\n",
    "<li>Tokenization is the process of converting a text into smaller sub-texts, based on certain predefined rules. For example, sentences are tokenized to words (and punctuation optionally). And paragraphs into sentences, depending on the context</li>\n",
    "    </ul>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the tokens (10 of them)\n",
    "i = 0\n",
    "for token in doc1:\n",
    "    if (i == 10):\n",
    "        break\n",
    "    print(token.text)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='gray'>\n",
    "<h3>\n",
    "<span style=\"font-family:verdana; font-size:1.2em;\">\n",
    "spaCy - Preprocessing<br>\n",
    "</span>\n",
    "</h3>\n",
    "</font>\n",
    "<span style=\"font-family:verdana; font-size:1.2em;\">\n",
    "    <ul>\n",
    "        <li>Remove stop words such as the, was, it, etc</li>\n",
    "        <li>Remove punctuations, extra spaces</li>\n",
    "    </ul>\n",
    "    This helps in reducing the amount of words that needs to be processed\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print if token is stop word or punctuation\n",
    "for token in doc1:\n",
    "    print(token.text, '\\t', token.is_stop, '\\t', token.is_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the stop words and punctuations\n",
    "doc1_clean = [token for token in doc1 if not token.is_stop and not token.is_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc1_clean:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Length: Original text {} preprocessed text {}\".format(len(text1), len(doc1_clean)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='gray'>\n",
    "<h3>\n",
    "<span style=\"font-family:verdana; font-size:1.2em;\">\n",
    "spaCy - Lemmatization<br>\n",
    "</span>\n",
    "</h3>\n",
    "</font>\n",
    "<span style=\"font-family:verdana; font-size:1.2em;\">\n",
    "    <ul>\n",
    "        <li>Lemmatization is finding the root of the word e.g. played, playing, plays have a root work play</li>\n",
    "        <li>Useful when dealing with number of occurances of a word - playing, plays are same as play</li>\n",
    "    </ul>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = 'she played chess against mary she likes playing chess.'\n",
    "doc2 = model(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the lemma of the words\n",
    "# NOTE: pronouns are identified here, they need to be handled separately\n",
    "for token in doc2:\n",
    "    print(token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='gray'>\n",
    "<h3>\n",
    "<span style=\"font-family:verdana; font-size:1.2em;\">\n",
    "spaCy - Vocab<br>\n",
    "</span>\n",
    "</h3>\n",
    "</font>\n",
    "<span style=\"font-family:verdana; font-size:1.2em;\">\n",
    "    <ul>\n",
    "        <li>Words of a doc are stored in Vocab</li>\n",
    "        <li>These words are converted to unique id</li>\n",
    "        <li>Can do look up between the id and the word and vice versa</li>\n",
    "        <li>Word will have the same hash value irrespective of the document</li>\n",
    "    </ul>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid = model.vocab.strings['chess']\n",
    "print(uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.vocab.strings[uid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='gray'>\n",
    "<h3>\n",
    "<span style=\"font-family:verdana; font-size:1.2em;\">\n",
    "spaCy - Lexical attributes<br>\n",
    "</span>\n",
    "</h3>\n",
    "</font>\n",
    "<span style=\"font-family:verdana; font-size:1.2em;\">\n",
    "    Previous examples used is_punct, is_space, etc. These are Lexical attributes.<br>\n",
    "    There are many lexical attibutes that are found:\n",
    "    <ul>\n",
    "        <li>like_num</li>\n",
    "        <li>like_email</li>\n",
    "        <li>like_url</li>\n",
    "        <li>etc...</li>\n",
    "    </ul>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = '2020 is far worse for the world economy than 2009'\n",
    "doc3 = model(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get numerical values\n",
    "for token in doc3:\n",
    "    if token.like_num:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text4 = \"\"\" name : Jim age: 45 email : jsmith@gmail.com\n",
    "                 name : John age: 34 email: jdoe8888@gmail.com\n",
    "                 name : Nila age: 60 email : nwafers222@gmail.com\n",
    "                 name : Mary age: 15 email : mpotter@yahoo.com\n",
    "                 \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc4 = model(text4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc4:\n",
    "    if token.like_email:\n",
    "        print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='gray'>\n",
    "<h3>\n",
    "<span style=\"font-family:verdana; font-size:1.2em;\">\n",
    "spaCy - Part of Speech Analysis<br>\n",
    "</span>\n",
    "</h3>\n",
    "</font>\n",
    "<span style=\"font-family:verdana; font-size:1.2em;\">\n",
    "    Spacy helps recognize different parts of speech - nouns, verbs, pronouns, etc.<br>\n",
    "    POS can be used to remove certain text \"junk\" like etc, i.e.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text5 = 'John plays basketball,if time permits. He played in high school too'\n",
    "doc5 = model(text5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc5:\n",
    "    print(token.text, '\\t', token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# understand what POS means\n",
    "spacy.explain('SCONJ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove \"junk\" text using POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text6 = \"\"\"I liked the movies etc The movie had good direction  The movie was amazing i.e.\n",
    "            The movie was average direction was not bad The cinematography was nice. i.e.\n",
    "            The movie was a bit lengthy  otherwise fantastic  etc etc\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc6 = model(text6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the \"junk\" text\n",
    "for token in doc6:\n",
    "    if token.pos_ == \"X\":\n",
    "        print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove junk text\n",
    "doc6_clean = [token for token in doc6 if not token.pos_ == \"X\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc6_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of parts of speech and correctponding token numbers\n",
    "doc6_tags = {token.pos: token.pos_ for token in doc6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc6_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of POS using displacy\n",
    "from spacy import displacy\n",
    "displacy.render(doc5, style = 'dep', jupyter = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='gray'>\n",
    "<h3>\n",
    "<span style=\"font-family:verdana; font-size:1.2em;\">\n",
    "spaCy - Named Entity Recognition<br>\n",
    "</span>\n",
    "</h3>\n",
    "</font>\n",
    "<span style=\"font-family:verdana; font-size:1.2em;\">\n",
    "    Spacy allows to find named entities in the text e.g. \"John works for Cisco\", John and Cisco are named entities - person and company\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text7 = 'Tony Stark owns the company StarkEnterprises. Emily Clark works at Microsoft and lives in Manchester. She loves to read the Bible and learn French'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc7 = model(text7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc7.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the type of entity\n",
    "for entity in doc7.ents:\n",
    "    print(entity.text, '\\t', entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc7, style = 'ent', jupyter = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
