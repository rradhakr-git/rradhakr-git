{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='steelblue'>\n",
    "\n",
    "<span style=\"font-family:verdana; font-size:1.6em;\">\n",
    "    <strong>Logistic Regression Example</strong><br><br>\n",
    "    Use Income Prediction dataset and apply Data Preprossor to process data<br><br>\n",
    "</span>\n",
    "<span style=\"font-family:verdana; font-size:1.4em;\">\n",
    "    <b>Following examples are included in the processing:</b><em>\n",
    "    <ol>\n",
    "        <li>Load dataset</li>\n",
    "        <li>Convert target column from string to numbers</li>\n",
    "        <li>Build a pipeline for data processing</li>\n",
    "        <li>Apply the pipeline to the dataframe</li>\n",
    "        <li>Build a Logistic Regression Model</li>\n",
    "        <li>Explore trained model performance</li>\n",
    "        <li>Make predictions using test dataset</li>\n",
    "        <li>Explore model performance using Confusion Matrix</li>\n",
    "        <li>Persist the pipeline and model</li>\n",
    "    </ol></em>\n",
    "</span>\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Adult dataset we are going to use is publicly available at the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Adult).\n",
    "This data derives from census data, and consists of information about individuals and their annual income.\n",
    "We will use this information to predict if an individual earns **<=50K or >50k** a year.\n",
    "The dataset is rather clean, and consists of both numeric and categorical variables.\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "- age: continuous\n",
    "- workclass: Private,Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked\n",
    "- fnlwgt: continuous\n",
    "- education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc...\n",
    "- education-num: continuous\n",
    "- marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent...\n",
    "- occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners...\n",
    "- relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried\n",
    "- race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black\n",
    "- sex: Female, Male\n",
    "- capital-gain: continuous\n",
    "- capital-loss: continuous\n",
    "- hours-per-week: continuous\n",
    "- native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany...\n",
    "\n",
    "<b>Target/Label: - <=50K, >50K</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')    # grids in the plots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from the datasets directory (agent.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/agent.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the income column\n",
    "<span style=\"font-family:times, serif; font-size:16pt; font-style:bold\">\n",
    "Convert the income from string to number using LabelEncoder\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "df['income'] = labelencoder.fit_transform(df.income)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['income'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Categorical columns\n",
    "<span style=\"font-family:Arial; font-size:14pt; font-style:bold\">\n",
    "    Building a pipeline to process data: \n",
    "<ol>\n",
    "<li>For numerical columns - replace missing values with median</li>\n",
    "<li>Then apply standard scaler to standardize these columns</li>\n",
    "<li>For the categorical column fill any missing value with constant</li>\n",
    "<li>Then perform One Hot Encoding (similar to pd_dummies)</li>\n",
    "</ol>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline stages for numeric and categorical columns\n",
    "numericPipe = Pipeline(steps=[('imputer', SimpleImputer(strategy='mean')),\n",
    "                              ('scaler', StandardScaler())])\n",
    "stringPipe = Pipeline(steps=[('imputer', SimpleImputer(strategy='constant', \n",
    "                                                       fill_value='missing')),\n",
    "                             ('onehot', OneHotEncoder(handle_unknown='ignore'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of numeric and categorical columns\n",
    "numericCols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "stringCols = df.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringCols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "    <h3>income column (int32) is not in any list</h3>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the pipeline\n",
    "<span style=\"font-family:verdana; font-size:14pt; font-style:bold\">\n",
    "    Processing the pipeline \n",
    "<ol>\n",
    "<li>Use Column Transformer to define the numeric and categorical transformers defined above</li>\n",
    "<li>Fit and Transform the dataframe</li>\n",
    "</ol>\n",
    "    \n",
    "<i>ColumnTransformer transformers to columns of an array or pandas DataFrame.\n",
    "\n",
    "This estimator allows different columns or column subsets of the input to be transformed separately and the features generated by each transformer will be concatenated to form a single feature space. This is useful for heterogeneous or columnar data, to combine several feature extraction mechanisms or transformations into a single transformer.</i>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "preprocessor = ColumnTransformer(transformers=[('num', numericPipe, numericCols),\n",
    "                                               ('cat', stringPipe, stringCols)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = preprocessor.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df1\n",
    "y = df['income'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.40, \n",
    "                                                    random_state = 2345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Logistic Regression to build model\n",
    "<span style=\"font-family:times, serif; font-size:14pt; font-style:bold\">\n",
    "    Build a model using the Logistic Regression algorithm\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logReg = LogisticRegression(solver = 'lbfgs', random_state = 2345)\n",
    "logReg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Training Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='teal'><h2>ROC Curve</h2></font>\n",
    "<ul>\n",
    "<span style=\"font-family:Arial; font-size:16pt; font-style:italic\">\n",
    "    <li>A visual way to measure the performance of binary classifier ROC (Receiver Operating Characteristic) Curve</li>\n",
    "    <li>Created by plotting True Positive Rate (TPR or recall) against False Positive Rate (FPR)</li>\n",
    "\n",
    "</span></ul>\n",
    "\n",
    "\n",
    "<font color='teal'><h2>AUC - Area Under the ROC curve</h2></font>\n",
    "\n",
    "<ul>\n",
    "    <span style=\"font-family:Arial; font-size:16pt; font-style:italic\">\n",
    "    <li>AUC is a good measure of performance of the classifier</li>\n",
    "    <li>If it is near 0.5, the classifier is not much better than random guessing</li>\n",
    "    <li>Classifier gets better when the curve get close to 1</li>\n",
    "    <li>Since our value is close to 1, it indicates that classifier is good\n",
    "at minimizing false negatives (not purchased as purchased) and true negative\n",
    "(purchased is classified as purchased)</li> \n",
    "\n",
    "</span></ul>\n",
    "<font color='tomato'><h2>ONLY VALID FOR BINARY CLASSIFICATION</h2></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logReg_auc = roc_auc_score(y_test, logReg.predict(X_test))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logReg.predict_proba(X_test)[:,1])\n",
    "\n",
    "auc = str(np.round(logReg_auc, 4))\n",
    "plt.figure(figsize = (10, 6))\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random guess', color = 'red')\n",
    "plt.plot(fpr, tpr, label = \"Train AUC \" + auc)\n",
    "plt.ylabel('False Positive Rate', fontsize = 16)\n",
    "plt.xlabel('True Positive Rate', fontsize = 16)\n",
    "plt.title('Receiver Operating Curve', fontsize = 16)\n",
    "plt.legend(loc = 4, fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on the test data and save them\n",
    "y_pred = logReg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score how good our model performedIMp\n",
    "logReg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Acurate and Error prediction counts\n",
    "correct = cm[0, 0] + cm[1, 1]\n",
    "error = cm[0, 1] + cm[1,0]\n",
    "total = correct + error\n",
    "print('Correct predictions: {} of {}'.format(correct, total))\n",
    "print('Errored predictions: {} of {}'. format(error, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cseg = [\"<= 50K\", \"> 50K\"]\n",
    "cm_df = pd.DataFrame(cm, index = cseg, columns = cseg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix\n",
    "import seaborn as sns\n",
    "plt.figure(figsize = (10, 6))\n",
    "sns.heatmap(cm_df, annot=True, cmap=plt.cm.Blues, fmt = 'g', annot_kws={\"size\": 16})\n",
    "sns.set(font_scale=0.5)\n",
    "plt.title('Confusion Matrix\\n', fontsize = 18)\n",
    "plt.ylabel('True label', fontsize = 16)\n",
    "plt.xlabel('Predicted label', fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist the preprocessor so that it can be reused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "dump(preprocessor, open('../preprocessor.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presist the model\n",
    "<span style=\"font-family:times, serif; font-size:14pt; font-style:bold\">\n",
    "    Persist the model using joblib library\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(logReg, '../logRegModel.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "name": "binary-classification",
  "notebookId": 3873517491194609
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
